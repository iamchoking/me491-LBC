seed: 1
record_video: yes

training:
  lr_start: 0 # 0 for no change

environment:
  render: True
#  for training
  num_envs: 50
  eval_every_n: 30
  record_every_n: 3
  max_time: 10.01 # max_time larger than 10 is required to observe draws.

#  for visualizing
#  num_envs: 1
#  eval_every_n: 1
#  max_time: 1000

  num_threads: 32
  simulation_dt: 0.0025
  control_dt: 0.01
  action_std: 0.3

  reward:
    class:
      coeff: 0.1   # !time and class must have exact opposite coeffs!
    time:
      coeff: -0.1  # !time and class must have exact opposite coeffs!

#    [STD]
##    PENALIZE
    slip:
      coeff: -0.8
    flight:
      coeff: -0.2
    tilt:
      coeff: -0.3
    edge:
      coeff: -0.3

#    PROMOTE
    face:
      coeff: 0.6
    ram:
      coeff: 1
    center:
      coeff: 0
    tbone:
      coeff: 0.2

#        [FREE-FOR-ALL] (-> just win)
##        PENALIZE
#    slip:
#      coeff: 0
#    flight:
#      coeff: 0
#    tilt:
#      coeff: 0
#    edge:
#      coeff: 0
#
##        PROMOTE
#    face:
#      coeff: 0
#    ram:
#      coeff: 0
#    center:
#      coeff: 0


  reward_win:              10
  reward_lose:             -10
  reward_draw:             -5

  training_mode:           0 #0: cube, 1: self-train
  training_init_shuffle:   True
  training_dummy_opponent: False

  curriculum_mass_start:   0.5
  curriculum_win_streak:   10
  curriculum_mass_incr:    0.1
  curriculum_cube_shuffle: True
  curriculum_head_shuffle: True

  stability_mode:          True
  stability_cube_teleport: -1
  stability_win_decay:     1.1


evaluation:
  iteration: 10000 #Write your final submit policy iteration. ex) if you submit full_10000.pt

architecture:
  policy_net: [128, 128]
  value_net: [128, 128]
