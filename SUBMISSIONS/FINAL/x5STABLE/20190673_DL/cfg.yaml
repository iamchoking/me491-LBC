seed: 1
record_video: yes

environment:
  render: True
  #  for training
  num_envs: 50
  eval_every_n: 30
  max_time: 10.01 # max_time larger than 10 is required to observe draws.


  plot_metric_n: 10

  num_threads: 32
  simulation_dt: 0.0025
  control_dt: 0.01
  action_std: 0.3

  reward:
    time:
      coeff: 0.0

    terminal: # used in stability training
      coeff: 0.0

    #    [STD]
    ##    PENALIZE
    slip:
      coeff: 0.0
    flight:
      coeff: 0.0
    tilt:
      coeff: 0.0
    spin:
      coeff: 0.0
    edge:
      coeff: 0.0

    #    PROMOTE
    face:
      coeff: 0.0
    ram:
      coeff: 0.0
    center:
      coeff: 0.3
    tbone:
      coeff: 0.0
    away:
      coeff: 0.0
    opp_off_center:
      coeff: 0.0


  reward_win:              100
  reward_lose:             -100
  reward_draw_win :        -50
  reward_draw_lose:        -50

  training_mode:           1 #0: cube, 1: self-train
  training_init_shuffle:   True
  training_dummy_opponent: False

  curriculum_mass_start:   16
  curriculum_win_streak:   10
  curriculum_mass_incr:    0.1
  curriculum_cube_shuffle: True

  stability_mode:          True
  stability_teleport:      2.0
  stability_win_decay:     0.9


evaluation:
  iteration: 6480 #Write your final submit policy iteration. ex) if you submit full_10000.pt

architecture:
  policy_net: [128, 128]
  value_net: [128, 128]
